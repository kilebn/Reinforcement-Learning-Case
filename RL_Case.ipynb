{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"B3MZThDY-Qcz"},"outputs":[],"source":["!pip install gym[classic_control]==0.26.2\n","!pip install stable-baselines3[extra]==2.0.0"]},{"cell_type":"markdown","metadata":{"id":"6Xan_3gqBvmj"},"source":[" --- Code to Clear Saved Data and Videos ---\n","\n","\"\"\"\n","# Code to Clear Saved Data and Videos\n","\n","Run this cell if you want to remove the directories and files created\n","by the previous training and demonstration steps.\n","\n","This will delete:\n","- The log directory (./tmp/gym) containing monitor logs and saved models (best_model.zip and checkpoints).\n","- The video directories (untrained_agent_video, trained_agent_video_intermediate, trained_agent_video_best).\n","\n","**Warning:** This action is irreversible!\n","\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QyptHmjOByKR"},"outputs":[],"source":["import os\n","import shutil\n","\n","# Define the paths to be cleared (ensure these match the paths used in the main script)\n","log_dir = \"./tmp/gym/\"\n","video_folder_untrained = \"untrained_agent_video\"\n","video_folder_intermediate = \"trained_agent_video_intermediate\"\n","video_folder_best = \"trained_agent_video_best\"\n","\n","# List of directories to remove\n","directories_to_clear = [\n","    log_dir,\n","    video_folder_untrained,\n","    video_folder_intermediate,\n","    video_folder_best\n","]\n","\n","print(\"--- Clearing Directories and Files ---\")\n","\n","for directory in directories_to_clear:\n","    if os.path.exists(directory):\n","        print(f\"Removing directory: {directory}\")\n","        try:\n","            shutil.rmtree(directory)\n","            print(f\"Successfully removed {directory}\")\n","        except OSError as e:\n","            print(f\"Error removing directory {directory}: {e}\")\n","    else:\n","        print(f\"Directory not found, skipping: {directory}\")\n","\n","print(\"--- Clearing Complete ---\")"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"LrPJPZ8fABZ1","executionInfo":{"status":"ok","timestamp":1748529468988,"user_tz":-120,"elapsed":2,"user":{"displayName":"Kilian Ebner","userId":"02316212586031058623"}}},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","RL Demo: CartPole with Stable-Baselines3 in Colab\n","\n","This script demonstrates basic Reinforcement Learning concepts\n","live environment.\n","\n","It covers:\n","1. Setting up the environment (CartPole).\n","2. Showing an untrained agent's behavior.\n","3. Explaining the State, Action, and critically, the Reward function.\n","4. Training an agent using A2C from Stable-Baselines3.\n","5. Visualizing the training progress (reward curve).\n","6. Showing the trained agent's improved behavior.\n","\"\"\"\n","\n","# --- Imports ---\n","import os\n","import base64\n","from typing import Callable\n","# Import time for optional frame delay (not strictly necessary for CartPole)\n","import time\n","\n","import gymnasium as gym\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# Import imageio for manual video saving\n","import imageio\n","import math\n","\n","from stable_baselines3 import A2C\n","from stable_baselines3.common.evaluation import evaluate_policy\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n","from stable_baselines3.common.results_plotter import ts2xy, load_results\n","\n","\n","# Colab specific imports for video display\n","from IPython import display\n","from IPython.display import HTML\n","\n","# --- Colab Setup ---\n","# This cell installs necessary libraries. Run this first.\n","# %pip install gymnasium stable-baselines3 stable-baselines3[extra] ipython\n","\n","# Create necessary directories\n","log_dir = \"./tmp/gym/\"\n","os.makedirs(log_dir, exist_ok=True)\n","\n","# Helper function to display videos in Colab\n","def show_video(video_path, width=600):\n","    \"\"\"Helper function to display video in Colab\"\"\"\n","    if not os.path.exists(video_path):\n","        print(f\"Video file not found: {video_path}\")\n","        return\n","\n","    mp4 = open(video_path, 'rb').read()\n","    data_url = \"data:video/mp4;base64,\" + base64.b64encode(mp4).decode()\n","    display.display(HTML(f'<video width=\"{width}\" controls><source src=\"{data_url}\" type=\"video/mp4\"></video>'))\n","\n","def find_videos(directory, prefix=\"rl-video-episode-\"):\n","    \"\"\"Find video files generated by RecordVideo in a directory.\"\"\"\n","    # RecordVideo saves files like 'rl-video-episode-0.mp4', 'rl-video-episode-1.mp4', etc.\n","    # or uses the name_prefix if provided: 'your_prefix-episode-0.mp4'\n","    # Adjusting the prefix search slightly to be more general or specific\n","    video_files = sorted([f for f in os.listdir(directory) if f.endswith(\".mp4\") and prefix in f])\n","    # You might need to adjust the prefix check if RecordVideo naming changes,\n","    # but `prefix in f` is generally safer than `f.startswith(prefix)`\n","    return [os.path.join(directory, f) for f in video_files]"]},{"cell_type":"markdown","metadata":{"id":"X763QoorAJ9i"},"source":["# --- Section 1: Introduction to the Environment (CartPole) ---\n","\n","Welcome to this demonstration of Reinforcement Learning! We'll use a classic environment called **CartPole**.\n","\n","Imagine a cart on a track, with a pole attached to a pivot on top. The goal is to keep the pole upright by moving the cart left or right.\n","\n","- **State:** What information does our agent receive?\n","    - Position of the cart\n","    - Velocity of the cart\n","    - Angle of the pole\n","    - Angular velocity of the pole\n","    This is a continuous state space (floating-point numbers).\n","\n","- **Action:** What can our agent do?\n","    - Push the cart to the left\n","    - Push the cart to the right\n","    This is a discrete action space (just two actions).\n","\n","- **Episode:** A single attempt to balance the pole. It starts with the pole slightly off-center and ends when the pole falls too far, the cart goes off-screen, or a time limit (e.g., 500 steps) is reached.\n","\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1OfAdCTATcf"},"outputs":[],"source":["# Create the environment\n","# render_mode='rgb_array' is needed for recording video later\n","env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n","\n","print(\"--- Environment Details ---\")\n","print(f\"Observation Space (State): {env.observation_space}\")\n","print(f\"Action Space (Actions): {env.action_space}\")\n","print(\"-------------------------\")\n","\n","# Reset the environment to get the initial state\n","obs, info = env.reset()\n","print(f\"Initial State:\\n {obs}\")\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"lNhFgob_AXtW"},"source":["# --- Section 2: Understanding the Reward Function ---\n","\n","\n","This is a crucial part of RL! The reward function defines the goal.\n","\n","In CartPole:\n","- The agent receives a reward of **+1 for every single timestep** the pole remains upright.\n","\n","Think about this:\n","- If the pole falls after 50 steps, the total reward for that episode is 50.\n","- If the pole stays upright for 100 steps, the total reward is 100.\n","- If the pole stays upright for the maximum allowed steps (e.g., 500), the total reward is 500.\n","\n","What behavior does this reward function encourage?\n","-> Staying upright for as long as possible!\n","-> Maximizing the cumulative reward over an episode.\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"J5WeQVnlAj4y"},"source":["# --- Section 3: Demonstrating an Untrained Agent (Random Actions) ---\n","\n","Before training, let's see how the environment behaves with an agent that does nothing intelligent â€“ it just takes random actions.\n","\n","We expect it to fail quickly, as there's no strategy involved.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nuhNSLSsAjMH"},"outputs":[],"source":["# Create a new environment instance for demonstration\n","demo_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n","\n","# --- Modification to show pole falling further ---\n","# Access the underlying environment object (unwrap it if wrappers are applied)\n","unwrapped_env = demo_env.unwrapped\n","# Change the angle threshold for termination\n","# Default is 12 degrees (0.2094 radians). Let's increase it to 20 degrees.\n","new_angle_threshold_degrees = 90\n","unwrapped_env.theta_threshold_radians = new_angle_threshold_degrees * math.pi / 180\n","unwrapped_env.x_threshold = 3.0\n","print(f\"--- Temporarily increased pole angle threshold for demo to {new_angle_threshold_degrees} degrees ---\")\n","# -------------------------------------------------\n","\n","\n","video_folder = \"untrained_agent_video\"\n","os.makedirs(video_folder, exist_ok=True)\n","\n","# Record the first N episodes for demonstration\n","n_untrained_episodes = 3 # Record 3 episodes of the random agent\n","# The video_length=500 ensures we record up to the max steps per episode,\n","# but the episode will terminate earlier due to the modified angle threshold.\n","demo_env = gym.wrappers.RecordVideo(demo_env, video_folder, episode_trigger=lambda x: x < n_untrained_episodes, video_length=500) # Record first n episodes\n","\n","print(f\"--- Running Untrained Agent (Random Actions) for {n_untrained_episodes} episodes ---\")\n","\n","for episode in range(n_untrained_episodes):\n","    # Note: The RecordVideo wrapper automatically calls reset and handles the episode loop internally\n","    # when used with evaluate_policy or manual steps like below.\n","    # However, since we want to capture full episodes *ending* in failure visually,\n","    # we'll manually step through one episode per recorded video.\n","    # The episode_trigger and video_length handle starting/stopping recording.\n","\n","    # Reset the environment (this is handled by the wrapper and the first step below,\n","    # but explicit reset is good practice if not using evaluate_policy)\n","    obs, info = demo_env.reset(seed=42 + episode) # Use different seed for each episode for variation\n","\n","    total_reward = 0\n","    step_count = 0\n","    terminated = False\n","    truncated = False\n","\n","    print(f\"\\n--- Episode {episode + 1} ---\")\n","\n","    # Run one episode until termination or truncation (with the MODIFIED threshold)\n","    while not (terminated or truncated):\n","        action = demo_env.action_space.sample() # Take a random action\n","        obs, reward, terminated, truncated, info = demo_env.step(action)\n","        total_reward += reward\n","        step_count += 1\n","\n","    print(f\"Episode finished after {step_count} steps.\")\n","    print(f\"Total reward: {total_reward}\")\n","    print(f\"Termination status: Terminated={terminated}, Truncated={truncated}\")\n","    print(\"--------------------------\")\n","\n","demo_env.close() # Close the wrapped environment\n","\n","# Display the recorded videos\n","print(f\"\\n--- Untrained Agent Videos ({n_untrained_episodes} episodes) ---\")\n","video_files = sorted([f for f in os.listdir(video_folder) if f.endswith(\".mp4\")])\n","if video_files:\n","    for i, video_file in enumerate(video_files):\n","        # Check if the video file is reasonably sized before trying to display\n","        # Sometimes RecordVideo can produce tiny files if it errors or stops immediately\n","        if os.path.getsize(os.path.join(video_folder, video_file)) > 1000: # Simple check\n","             print(f\"Video {i+1}/{len(video_files)}:\")\n","             show_video(os.path.join(video_folder, video_file))\n","        else:\n","             print(f\"Skipping display for potentially empty or very small video: {video_file}\")\n","else:\n","    print(\"No videos were successfully recorded for the untrained agent.\")"]},{"cell_type":"markdown","metadata":{"id":"Ykc33mzXAtxb"},"source":["# --- Section 4: Choosing an Algorithm and Training Setup ---\n","\n","To train an agent, we need an algorithm. There are many!\n","\n","We'll use **A2C (Advantage Actor-Critic)** from the Stable-Baselines3 library.\n","- A2C is an Actor-Critic algorithm.\n","    - **Actor:** Learns a *policy* (a strategy) - mapping states to actions.\n","    - **Critic:** Learns a *value function* - estimating how good a state is (expected future reward).\n","- They learn together, using the critic's estimate to improve the actor's policy.\n","\n","Stable-Baselines3 provides robust implementations. We'll use the `A2C` class with a simple `MlpPolicy` (Multi-Layer Perceptron neural network) as the function approximator for both actor and critic.\n","\n","To monitor training, we'll use an `EvalCallback`. This callback periodically evaluates the agent on a separate environment and logs the performance (like average reward). This is essential for seeing if the agent is actually learning.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJUqNFwJAviX"},"outputs":[],"source":["# Define evaluation parameters\n","eval_freq = 1000  # Evaluate every 1000 timesteps\n","n_eval_episodes = 10 # Evaluate over 10 episodes\n","eval_env_id = 'CartPole-v1'\n","\n","# Create a separate evaluation environment (uses DEFAULT CartPole-v1)\n","# Wrap the eval env with Monitor to record episode stats for the EvalCallback\n","eval_env = make_vec_env(eval_env_id, n_envs=1, seed=42)\n","unwrapped_env = demo_env.unwrapped\n","# Change the angle threshold for termination\n","# Default is 12 degrees (0.2094 radians). Let's increase it to 20 degrees.\n","new_angle_threshold_degrees = 90\n","unwrapped_env.theta_threshold_radians = new_angle_threshold_degrees * math.pi / 180\n","unwrapped_env.x_threshold = 3.0\n","eval_env = Monitor(eval_env.envs[0], log_dir) # Monitor needs a single env\n","\n","# Create the EvalCallback\n","eval_callback = EvalCallback(eval_env,\n","                             best_model_save_path=log_dir,\n","                             log_path=log_dir,\n","                             eval_freq=eval_freq,\n","                             n_eval_episodes=n_eval_episodes,\n","                             deterministic=True, # Use deterministic actions during eval\n","                             render=False) # No rendering during evaluation training\n","\n","# Create the CheckpointCallback\n","# Save model every 'save_freq' timesteps\n","save_freq = 1000 # Save a checkpoint every 5000 steps\n","# Ensure the save directory exists\n","os.makedirs(log_dir + \"/checkpoints\", exist_ok=True)\n","checkpoint_callback = CheckpointCallback(save_freq=save_freq, save_path=log_dir + \"/checkpoints/\",\n","                                         name_prefix=\"rl_model\")"]},{"cell_type":"markdown","metadata":{"id":"HaMGV3-BA0D0"},"source":["# --- Section 5: Training the Agent ---\n","\n","Now, let's train the A2C agent.\n","\n","We'll train for a set number of timesteps. Watch the output (if verbose > 0) or simply wait for it to complete. The `EvalCallback` will print messages periodically showing the evaluation results.\n","\n","The `learn()` method is where the magic happens - the agent interacts with the environment, collects data, and updates its policy and value function using the A2C algorithm.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycfNopGaA5Ih"},"outputs":[],"source":["# Create the A2C model (uses DEFAULT CartPole-v1 parameters)\n","# Policy: MlpPolicy\n","# Environment: CartPole-v1\n","# verbose=0 to keep output clean during lecture, set to 1 or 2 for more details\n","# seed for reproducibility\n","model = A2C(\"MlpPolicy\", \"CartPole-v1\", verbose=0, seed=0)\n","\n","print(\"\\n--- Training the Agent ---\")\n","# Train the model\n","# Pass both callbacks\n","total_timesteps = 8000 # Number of steps to train for. 10k-20k is usually enough for CartPole to solve.\n","print(f\"Training for {total_timesteps} timesteps...\")\n","model.learn(total_timesteps=total_timesteps, callback=[eval_callback, checkpoint_callback]) # Use a list for multiple callbacks\n","print(\"Training finished.\")\n","print(\"--------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"szrz9_3DBP3s"},"source":["# --- Section 6: Visualizing Training Progress ---\n","\n","Did the agent actually learn? We can see this by plotting the evaluation results collected by the `EvalCallback`.\n","\n","We'll plot the average reward obtained during evaluation episodes against the number of training timesteps. A successful learning process will show the average reward increasing over time.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zO1LwZ_bBPHZ"},"outputs":[],"source":["print(\"\\n--- Plotting Training Progress ---\")\n","\n","# Load the results from the log directory\n","# 'monitor.csv' is the file where Monitor/EvalCallback logs results\n","# Check if monitor.csv exists first\n","monitor_file_path = os.path.join(log_dir, 'monitor.csv')\n","if os.path.exists(monitor_file_path):\n","    # ts2xy extracts the timesteps (x) and mean rewards (y)\n","    x, y = ts2xy(load_results(log_dir), 'timesteps')\n","\n","    if len(x) > 0:\n","        # --- Smoothing the data ---\n","        smoothing_window = 10 # Define the window size for smoothing. Adjust as needed.\n","        # Use convolution to compute the moving average\n","        # Create a window (a list of 1/window_size)\n","        window = np.ones(smoothing_window) / smoothing_window\n","        # Convolve the data with the window\n","        y_smoothed = np.convolve(y, window, mode='valid') # 'valid' mode returns convolution where window fully overlaps\n","        # The corresponding x values are shifted because of 'valid' mode.\n","        # A simple approach is to align the smoothed points with the end of the window,\n","        # but for visualization, aligning with the start or center or just taking\n","        # the corresponding range of x values is common. Let's align with the end of the window.\n","        x_smoothed = x[smoothing_window - 1:]\n","        # --------------------------\n","\n","\n","        plt.figure(figsize=(10, 6))\n","        # Plot the smoothed data\n","        plt.plot(x_smoothed, y_smoothed)\n","        # Optional: Plot raw data underneath with transparency for context\n","        # plt.plot(x, y, alpha=0.3, label='Raw')\n","        plt.xlabel(\"Number of Timesteps\")\n","        plt.ylabel(f\"Smoothed Mean Episode Reward (window={smoothing_window})\")\n","        plt.title(\"Training Progress: CartPole-v1 (Smoothed)\")\n","        plt.grid(True)\n","        # plt.legend() # Uncomment if plotting raw data\n","        plt.show()\n","        print(\"Smoothed plot displayed above.\")\n","        print(f\"Smoothing window size: {smoothing_window}\")\n","    else:\n","        print(\"No data logged in monitor.csv to plot.\")\n","        print(f\"Check if the eval_callback saved results to: {monitor_file_path}\")\n","else:\n","    print(f\"Monitor file not found at: {monitor_file_path}\")\n","    print(\"Training evaluation data was not logged.\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"y4efHOkyBXoM"},"source":["# --- Section 7: Demonstrating the Trained Agent ---\n","\n","\n","Now for the moment of truth! Let's load the *best* model saved during training (based on evaluation performance) and see how it performs.\n","\n","We expect it to balance the pole for much longer than the random agent, ideally reaching the maximum number of steps per episode.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fpDVHTXD-SC0"},"outputs":[],"source":["print(\"\\n--- Demonstrating Trained Agents ---\")\n","\n","n_trained_demo_episodes = 3 # Number of episodes to demonstrate for each trained agent\n","max_episode_steps = 500 # Max steps for CartPole-v1\n","\n","# --- Demonstration of Intermediate Agent ---\n","intermediate_steps = 3000 # The timestep we want to load\n","intermediate_model_path = os.path.join(log_dir, \"checkpoints\", f\"rl_model_{intermediate_steps}_steps.zip\")\n","\n","if os.path.exists(intermediate_model_path):\n","    print(f\"\\n--- Intermediate Agent (at {intermediate_steps} steps) ---\")\n","    loaded_intermediate_model = A2C.load(intermediate_model_path)\n","    print(f\"Loaded intermediate model from: {intermediate_model_path}\")\n","\n","    # Create a new environment for the intermediate demo (uses DEFAULT CartPole-v1)\n","    demo_env_intermediate = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n","    unwrapped_env = demo_env_intermediate.unwrapped\n","    # Change the angle threshold for termination\n","    # Default is 12 degrees (0.2094 radians). Let's increase it to 20 degrees.\n","    new_angle_threshold_degrees = 90\n","    unwrapped_env.theta_threshold_radians = new_angle_threshold_degrees * math.pi / 180\n","    unwrapped_env.x_threshold = 3.0\n","    video_folder_intermediate = \"trained_agent_video_intermediate\"\n","    os.makedirs(video_folder_intermediate, exist_ok=True)\n","    # Record N episodes into potentially one video file (if wrapper supports, often separate files)\n","    # video_length ensures we record the full 500 steps if successful\n","    demo_env_intermediate = gym.wrappers.RecordVideo(\n","        demo_env_intermediate,\n","        video_folder_intermediate,\n","        episode_trigger=lambda x: x < n_trained_demo_episodes,\n","        video_length=max_episode_steps,\n","        name_prefix=\"intermediate_agent_episode\" # Custom prefix for clarity\n","    )\n","\n","    print(f\"Running intermediate agent for {n_trained_demo_episodes} episodes on standard CartPole-v1...\")\n","    # evaluate_policy handles running multiple episodes and interacting with the wrapper\n","    mean_reward_intermediate, std_reward_intermediate = evaluate_policy(\n","        loaded_intermediate_model,\n","        demo_env_intermediate,\n","        n_eval_episodes=n_trained_demo_episodes,\n","        deterministic=True # Use deterministic for consistent demo\n","    )\n","\n","    print(f\"\\nEvaluation results for intermediate agent over {n_trained_demo_episodes} episodes:\")\n","    print(f\"Mean reward: {mean_reward_intermediate:.2f} +/- {std_reward_intermediate:.2f}\")\n","    print(\"----------------------------------------------\")\n","\n","    demo_env_intermediate.close()\n","\n","    # Display the recorded videos for the intermediate agent\n","    print(f\"\\n--- Intermediate Agent Videos ({n_trained_demo_episodes} episodes) ---\")\n","    video_files_intermediate = find_videos(video_folder_intermediate, prefix=\"intermediate_agent_episode\")\n","    if video_files_intermediate:\n","        for i, video_file in enumerate(video_files_intermediate):\n","            if os.path.getsize(video_file) > 1000: # Simple check\n","                 print(f\"Video {i+1}/{len(video_files_intermediate)}:\")\n","                 show_video(video_file)\n","            else:\n","                 print(f\"Skipping display for potentially empty or very small video: {video_file}\")\n","    else:\n","         print(\"No videos recorded for the intermediate agent.\")\n","\n","else:\n","    print(f\"\\nIntermediate model not found at: {intermediate_model_path}\")\n","    print(f\"Unable to demonstrate the intermediate agent. Check if checkpoint at {intermediate_steps} steps was saved.\")\n","    print(\"----------------------------------------------\")\n","\n","\n","# --- Demonstration of Best Agent ---\n","best_model_path = os.path.join(log_dir, \"best_model.zip\")\n","if os.path.exists(best_model_path):\n","    print(f\"\\n--- Best Trained Agent ---\")\n","    loaded_best_model = A2C.load(best_model_path)\n","    print(f\"Loaded best model from: {best_model_path}\")\n","\n","    # Create a new environment for the best agent demo (uses DEFAULT CartPole-v1)\n","    demo_env_best = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n","    unwrapped_env = demo_env_best.unwrapped\n","    # Change the angle threshold for termination\n","    # Default is 12 degrees (0.2094 radians). Let's increase it to 20 degrees.\n","    new_angle_threshold_degrees = 90\n","    unwrapped_env.theta_threshold_radians = new_angle_threshold_degrees * math.pi / 180\n","    unwrapped_env.x_threshold = 3.0\n","    video_folder_best = \"trained_agent_video_best\"\n","    os.makedirs(video_folder_best, exist_ok=True)\n","    # Record N episodes\n","    demo_env_best = gym.wrappers.RecordVideo(\n","        demo_env_best,\n","        video_folder_best,\n","        episode_trigger=lambda x: x < n_trained_demo_episodes,\n","        video_length=max_episode_steps,\n","        name_prefix=\"best_agent_episode\" # Custom prefix\n","    )\n","\n","    print(f\"Running best agent for {n_trained_demo_episodes} episodes on standard CartPole-v1...\")\n","    # evaluate_policy handles running multiple episodes and interacting with the wrapper\n","    mean_reward_best, std_reward_best = evaluate_policy(\n","        loaded_best_model,\n","        demo_env_best,\n","        n_eval_episodes=n_trained_demo_episodes,\n","        deterministic=True # Use deterministic for consistent demo\n","    )\n","\n","    print(f\"\\nEvaluation results for best agent over {n_trained_demo_episodes} episodes:\")\n","    print(f\"Mean reward: {mean_reward_best:.2f} +/- {std_reward_best:.2f}\")\n","    print(\"----------------------------------------------\")\n","\n","    demo_env_best.close()\n","\n","    # Display the recorded videos for the best agent\n","    print(f\"\\n--- Best Agent Videos ({n_trained_demo_episodes} episodes) ---\")\n","    video_files_best = find_videos(video_folder_best, prefix=\"best_agent_episode\")\n","    if video_files_best:\n","        for i, video_file in enumerate(video_files_best):\n","            if os.path.getsize(video_file) > 1000: # Simple check\n","                 print(f\"Video {i+1}/{len(video_files_best)}:\")\n","                 show_video(video_file)\n","            else:\n","                 print(f\"Skipping display for potentially empty or very small video: {video_file}\")\n","    else:\n","         print(\"No videos recorded for the best agent.\")\n","\n","else:\n","    print(f\"\\nBest model not found at: {best_model_path}\")\n","    print(\"Unable to demonstrate the best agent.\")\n","    print(\"Please ensure training completed successfully and saved the best model.\")\n","    print(\"----------------------------------------------\")\n"]},{"cell_type":"markdown","metadata":{"id":"iaNbbL_gBapt"},"source":["# --- Conclusion ---\n","\n","\n","We've seen:\n","1. The CartPole environment: its state and action spaces.\n","2. The crucial role of the **Reward Function** (+1 per step upright).\n","3. How a random agent performs (poorly).\n","4. How an RL algorithm (A2C) learns by maximizing cumulative reward.\n","5. Visual evidence of learning through the reward curve.\n","6. The improved performance of the trained agent.\n","\n","This is a simple example, but the core concepts (State, Action, Reward, Policy, Value, Training to maximize cumulative reward) apply to much more complex problems like robotics, game playing, and more!\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}